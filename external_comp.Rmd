---
title: "Compatibility with other R packages"
output: github_document
---

```{r}
library(BayesMultiMode)
library(posterior)
```

## Bayesian estimation and mode inference
In the examples presented below external R packages are used for Bayesian estimation of mixture models while `BayesMultiMode` is used for mode inference.

### rjags
#### Estimation
```{r}
set.seed(123)

library(rjags)

# Assuming you have your data in 'data_vector'
# Specify the model
model_string = "
  model {
    for (i in 1:N) {
      data_vector[i] ~ dnorm(mu[component[i]], tau[component[i]])
      component[i] ~ dcat(theta[])
    }

    for (j in 1:K) {
      mu[j] ~ dnorm(0, 0.01)
      tau[j] ~ dgamma(1, 1)
    }

    theta ~ ddirch(rep(1, K))
  }
"

K = 2
# Data for JAGS
y = c(rnorm(100,0,1),
      rnorm(100,5,1))

data_jags = list(data_vector = y, N = length(y), K = K)

# Initialize parameters
inits = function() {
  list(mu = rnorm(K, 0, 10),
       tau = rgamma(K, 1, 1),
       theta = rep(1 / K, K))
}

# Run the model
model = jags.model(textConnection(model_string), data = data_jags, inits = inits)
update(model, 1000)  # Burn-in
fit = coda.samples(model, variable.names = c("mu", "tau", "theta"), n.iter = 5000)
```

#### Create a BayesMixture object
```{r}
fit_mat = as_draws_matrix(fit)
bmix = bayes_mixture(mcmc = fit_mat,
                        data = y,
                        burnin = 0, # the burnin has already been discarded
                        dist = "normal",
                        vars_to_keep = c("theta", "mu", "tau"),
                        vars_to_rename = c("eta" = "theta",
                                           "sigma" = "tau"))

# plot the mixture
plot(bmix)

# mode estimation
bayesmode = bayes_mode(bmix)

# plot mode inference
plot(bayesmode)

# Summary of mode inference
summary(bayesmode)
```

### rstan
#### Estimation
```{r}
set.seed(123)
library(rstan)

normal_mixture_model <- "
data {
  int<lower=0> N;         // number of data points
  real y[N];              // observed data
  int<lower=1> K;         // number of mixture components
}

parameters {
  simplex[K] theta;       // mixing proportions
  ordered[K] mu;          // means of the Gaussian components
  vector<lower=0>[K] sigma; // standard deviations of the components
}

model {
  vector[K] log_theta = log(theta); // cache log calculation
  
  // Priors
  mu ~ normal(0, 10);
  sigma ~ cauchy(0, 5);

  // Likelihood
  for (n in 1:N) {
    vector[K] log_lik;
    for (k in 1:K) {
      log_lik[k] = log_theta[k] + normal_lpdf(y[n] | mu[k], sigma[k]);
    }
    target += log_sum_exp(log_lik);
  }
}
"

y = c(rnorm(100,0,1),
      rnorm(100,5,1))

data_list <- list(
  N = length(y),
  y = y,
  K = 2  # Assuming a two-component mixture
)

fit <- stan(model_code = normal_mixture_model, data = data_list, iter = 2000, chains = 1)
```

#### Create a BayesMixture object
```{r}
fit_mat = as_draws_matrix(fit)
bmix = bayes_mixture(mcmc = fit_mat,
                        data = y,
                        burnin = 0, # the burnin has already been discarded
                        dist = "normal",
                        vars_to_keep = c("theta", "mu", "sigma"),
                        vars_to_rename = c("eta" = "theta"))

# plot the mixture
plot(bmix)

# mode estimation
bayesmode = bayes_mode(bmix)

# plot mode inference
plot(bayesmode)

# Summary of mode inference
summary(bayesmode)
```

### bayesmix
#### Estimation
```{r}
set.seed(123)
library(bayesmix)

## taken from the examples of JAGSrun()
data("fish", package = "bayesmix")
y = unlist(fish)
prefix <- "fish"
variables <- c("mu","tau","eta")
k <- 3
modelFish <- BMMmodel(k = k, priors = list(kind = "independence",
                                           parameter = "priorsFish", hierarchical = "tau"))
controlFish <- JAGScontrol(variables = c(variables, "S"),
                           n.iter = 2000, burn.in = 1000)
z1 <- JAGSrun(fish, prefix, model = modelFish, initialValues = list(S0 = 2),
              control = controlFish, cleanup = TRUE, tmp = FALSE)

fit = z1$results
```

#### Create a BayesMixture object
```{r}
# JAGSrun seems to report the variance (sigma2) rather the standard deviation so we need a new pdf :
pdf_func <- function(x, pars) {
  dnorm(x, pars["mu"], sqrt(pars["sigma"]))
}

bmix = bayes_mixture(mcmc = fit,
                        data = y,
                        burnin = 1000, 
                        pdf_func = pdf_func,
                        dist_type = "continuous",
                        loc = "mu",
                        vars_to_keep = c("eta", "mu", "sigma")) #vars_to_keep ignore numbers

# plot the mixture
plot(bmix)

# mode estimation
bayesmode = bayes_mode(bmix)

# plot mode inference
plot(bayesmode)

# Summary of mode inference
summary(bayesmode)
```


### BNPmix
#### Estimation
```{r}
set.seed(123)
library(BNPmix)
library(dplyr)

y = c(rnorm(100,0,1),
      rnorm(100,5,1))

## estimation
fit = PYdensity(y,
                mcmc = list(niter = 2000,
                            nburn = 1000,
                            print_message = FALSE),
                output = list(out_param = TRUE))
```

#### Transforming the output into a mcmc matrix with one column per variable
```{r, message = FALSE}
fit_mat = list()

for (i in 1:length(fit$p)) {
  k = length(fit$p[[i]][, 1])
  
  draw = c(fit$p[[i]][, 1],
           fit$mean[[i]][, 1],
           sqrt(fit$sigma2[[i]][, 1]),
           i)
  
  names(draw)[1:k] = paste0("eta", 1:k)
  names(draw)[(k+1):(2*k)] = paste0("mu", 1:k)
  names(draw)[(2*k+1):(3*k)] = paste0("sigma", 1:k)
  names(draw)[3*k + 1] = "draw"
  
  fit_mat[[i]] = draw
}

fit_mat = as.matrix(bind_rows(fit_mat))
```

#### Create a BayesMixture object
```{r}
bmix = bayes_mixture(mcmc = fit_mat,
                        data = y,
                        burnin = 0, # the burnin has already been discarded
                        dist = "normal",
                        vars_to_keep = c("eta", "mu", "sigma"))

# plot the mixture
plot(bmix)

# mode estimation
bayesmode = bayes_mode(bmix)

# plot mode inference
plot(bayesmode)

# Summary of mode inference
summary(bayesmode)
```


## Mode estimation in mixtures estimated with maximum likelihood
In the examples presented below external R package are used for estimating mixtures with the EM algorithm while `BayesMultiMode` is used for estimating and plotting modes.

### mixtools
```{r}
set.seed(123)
library(mixtools)

y = c(rnorm(100,0,1),
      rnorm(100,5,2))

fit = normalmixEM(y)

pars = c(eta = fit$lambda, mu = fit$mu, sigma = fit$sigma)

mix = mixture(pars, dist = "normal") # create a new object of class Mixture
modes = mix_mode(mix) # estimate modes and create an object of class Mode

plot(modes)
```

### mclust
```{r}
set.seed(123)
library(mclust)

y = c(rnorm(100,0,1),
      rnorm(100,5,2))

fit = Mclust(y)

pars = c(eta = fit$parameters$pro,
         mu = fit$parameters$mean,
         sigma = sqrt(fit$parameters$variance$sigmasq))

mix = mixture(pars, dist = "normal") # creates a new object of class Mixture
modes = mix_mode(mix) # estimates modes and creates an object of class Mode

plot(modes)
```

