---
title: "testing the estimation of continuous mixtures using stan"
output: html_document
---

# Load package
```{r}
library(BayesMultiMode)
```


# Data

## Galaxy data
```{r}
y = multimode::galaxyrg

plot(density(y), type="l")
```

## Normal draws
```{r, eval = FALSE}
mu = c(0.5,2,4)
sigma = c(0.3,0.4,0.2)
p = c(0.3,0.5,0.2)
nb_draws = 200

y = c(rnorm(nb_draws*p[1], mu[1], sigma[1]),
      rnorm(nb_draws*p[2], mu[2], sigma[2]),
      rnorm(nb_draws*p[3], mu[3], sigma[3]))

hist(y,main = "Simulated data", breaks=100)
```

## DNA data
```{r}
data("d4z4")
y = d4z4

##
p1 = 0.4
p2 = 1-p1
kap1 = 10
kap2 = 4
lam1 = 1
lam2 = 2
length_data = 50
y <- c(rpois(length_data*p1, lam1)+kap1, rpois(length_data*p2, lam2)+kap2)

##
# y = multimode::geyser[1:70]-45

hist(y)
```

## shifted negative binomial 
```{stan output.var="SNB"}
functions {
real SNB_lpmf(int y, real r, real mu) {
real log_f;

real mr = (1-r);
real a = mr/r;
real A = lgamma(y + a) - lgamma(y+1) - lgamma(a);
real B = ((1+r*(y-1))/r)*log(mr/(mr + mu*r));
real C = y*log(mu/a);
log_f = A + B + C;

return log_f; 
}
}

data {
int<lower=1> K;          // number of mixture components
int<lower=1> N;          // number of data points
int y[N];                // observations
real<lower=0> e0;        // prior - number of components
real<lower=0> e0_kappa;  // prior - distribution kappa
}

transformed data {
int max_y = max(y);
}

parameters {
simplex[K] theta;                                      // mixing proportions
vector<lower=0, upper = 0.5>[K] r;   
vector<lower=0, upper = max_y>[K] mu;
simplex[max_y+1] kappa[K];                             // location parameters}
}
// https://groups.google.com/g/stan-users/c/aN6eFv7Qmo8
// This block is necessary to integrate, or marginalise, kappa. This is the only way to deal with discrete parameters in 
// Stan; it is not possible to sample discrete parameters. 
transformed parameters {
vector[K] log_theta = log(theta);     // cache calculations on log mixture proportions.
vector[N] ll;

// positive_ordered[K] mu;
// mu = head(cumulative_sum(mu_0), K) * max_y;

for (n in 1:N) {
// for each observation

vector[K] lps;

for (k in 1:K) {
// for each mixture component
vector[max_y+1] log_kappa = log(kappa[k]);

vector[max_y+1] lp;
lp = rep_vector(log_theta[k], max_y+1) + log(kappa[k]);// + rep_vector(-log(max_y+1),max_y+1); //log(kappa[k]) + 
for (j in 0:max_y) {
// for each kappa parameter

if((y[n] - j) > -1){
lp[j+1] += SNB_lpmf(y[n] - j | r[k], mu[k]);
} else {
lp[j+1] = log(0);
}

}

lps[k] = log_sum_exp(lp);
}

ll[n] = log_sum_exp(lps);
}
}

model {
mu ~ uniform(0, max_y);
r ~ uniform(0, 0.5);

theta ~ dirichlet(rep_vector(e0, K));

for (i in 1:K) {
 kappa[i] ~ dirichlet(rep_vector(e0_kappa, max_y+1));
 //kappa[i] ~ uniform(0, max_y+1);
} 

target += sum(ll);
}
```

## normal
```{stan output.var="normal"}
data {
int<lower=1> K;          // number of mixture components
int<lower=1> N;          // number of data points
real y[N];               // observations
real<lower=0> e0;        // prior - number of components
real<lower=0> a0;        // prior - number of components
real<lower=0> A0;        // prior - number of components
real b0;                 // prior - mean
real<lower=0> B0;        // prior - mean
real<lower=0> c0;        // prior - variance 
real<lower=0> g0;        // prior - variance 
real<lower=0> G0;        // prior - variance 
int<lower = 0, upper = 1> SFM;
int<lower = 0, upper = 1> hier;
int<lower = 0, upper = 1> conj;
int<lower = 0, upper = 1> shrink;
}

parameters {
simplex[K] theta_0;   
//vector<lower=min(y), upper = max(y)>[K] mu;
//ordered[K] mu_raw;
real mu_raw[K];
vector<lower=0>[K] sigmaSQ;
vector<lower=0>[SFM ? 1 : 0] alpha;   // parameter mixing proportions
vector<lower=0>[hier ? K : 0] C0;      // see https://discourse.mc-stan.org/t/if-else-statement-inside-parameter-block/13937/3
vector<lower=0>[shrink ? 1 : 0] nu;   // parameter mixing proportions
}

transformed parameters {
vector<lower=0>[K] sigma;                // scales of mixture components
real mu[K];

simplex[K] theta;
theta = 0.0 + (1-0.0*K)*theta_0;

for (i in 1:K) {
sigma[i] = sqrt(sigmaSQ[i]);

if (conj) {
mu[i] = b0 + B0*sigma[i]*mu_raw[i];
} else if (shrink) {
mu[i] = b0 + B0*nu[1]*mu_raw[i];
} else if (shrink && conj) {
mu[i] = b0 + B0*nu[1]*sigma[i]*mu_raw[i];
} else {
mu[i] = b0 + B0*mu_raw[i];
}
}

}

model {
vector[K] log_theta;
vector[K] lps;

if (shrink) {
nu ~ gamma(0.5, 0.5); 
}

for (k in 1:K) {
if (hier) {
C0[k] ~ gamma(g0, G0);
sigmaSQ[k] ~ inv_gamma(c0, C0[k]);
} else {
sigmaSQ[k] ~ inv_gamma(c0, g0);
}
mu_raw[k] ~ std_normal();
}

if (SFM) {
alpha[1] ~ gamma(a0, A0);
theta_0 ~ dirichlet(rep_vector(alpha[1], K));
} else {
theta_0 ~ dirichlet(rep_vector(e0, K));
}

log_theta = log(theta);

for (n in 1:N) {
lps = log_theta;
for (k in 1:K) {
lps[k] += normal_lupdf(y[n] | mu[k], sigma[k]);
}
target += log_sum_exp(lps);
}
}
```

# R estimation functions using Stan
```{r, eval = T, warning = FALSE}
library(rstan)
K = 5

iter = 2000
data = y

fit <- sampling(
  object = normal,  # Stan program
  data = list(K = K,
              N = length(data),
              y = data,
              a0 = 10,
              A0 = 10*K,
              b0 = median(data),
              B0 = (max(data) - min(data))^2,
              c0 = 2.5,
              e0 = 1,#1/K,
              g0 = 0.5,
              G0 = 100*0.5/2.5/(max(data) - min(data))^2,#0.5/(sd(y)^2/2),
              hier = F,
              conj = F,
              SFM = T,
              shrink = F,
              e0 = 1,
              e0_kappa = 0.001
  ),
  # control = list(adapt_delta = 0.95),
  control = list(adapt_delta = 0.999, stepsize = 0.001, max_treedepth = 20),
  chains = 4,          
  warmup = iter/2,       
  iter = iter,         
  cores = 4,             
  refresh = iter/10           
)

# plot(fit)

rstan::check_hmc_diagnostics(fit)
```

# convergence
```{r, warning = FALSE}
library(magrittr)
# model_var = c("theta", "r", "mu")
model_var = c("theta", "mu", "sigma")
mcmc = posterior::as_draws_df(fit)
mcmc %<>% dplyr::select(tidyr::contains(model_var))
# mcmc[is.na(mcmc)] = 0
theme_p <- ggplot2::theme_minimal()
bayesplot::bayesplot_theme_set(theme_p)
j=15
col = c(wesanderson::wes_palettes[[j]], wesanderson::wes_palettes[[j]][2])
bayesplot::color_scheme_set(scheme = col[1:6])
for (par in model_var){
  show(bayesplot::mcmc_trace(mcmc,
                             pars = colnames(mcmc)[grep(par,colnames(mcmc))])) 
}

posterior::as_draws_df(fit) %>%
  tidyr::gather(-.chain, -.iteration, -.draw, key = "var", value = "value") %>%
  dplyr::group_by(.chain, var) %>%
  dplyr::summarise(rhat = posterior::rhat(value)) %>%
  tidyr::spread(key = "var", value = "rhat") %>%
  dplyr::select(tidyr::contains(c(model_var, "lp__"))) %>%
  dplyr::select(lp__)
# dplyr::ungroup() %>%
# dplyr::select(-.chain) %>%
# unlist() %>%
# hist(breaks = 30)

div = posterior::as_draws_df(fit) %>%
  tidyr::gather(-.chain, -.iteration, -.draw, key = "var", value = "value") %>%
  dplyr::group_by(.chain, var) %>%
  dplyr::summarise(rhat = posterior::rhat(value)) %>%
  tidyr::spread(key = "var", value = "rhat") %>%
  dplyr::select("lp__") %>%
  dplyr::filter(lp__>1.015) %>%
  dplyr::ungroup()
```

```{r, warning = FALSE}
mcmc = posterior::as_draws_matrix(fit)
mcmc = mcmc[, -grep("sigmaSQ", colnames(mcmc))]
mcmc = mcmc[, -grep("theta_0", colnames(mcmc))]

max_size = 200
tol_p = 0.0001
transparency = 0.1
colour_range = c("red", "blue", "green")
pars_names = c("theta", "mu", "sigma")

library(ggplot2)
g = ggplot(data.frame(y = y), aes(y)) +
  geom_histogram(aes_string(y = "..density.."),
                 colour = "white",
                 bins = 70)

## plot the mixture for each draw
for (i in sample(nrow(mcmc),min(nrow(mcmc), max_size))) {
  mcmc_i = mcmc[i, ]
  pars = c()
  for (j in 1:length(pars_names)) {
    pars = cbind(pars, mcmc_i[grep(pars_names[j], colnames(mcmc_i), fixed = T)])
  }
  
  colnames(pars) <- pars_names
  
  est_mode = rep(NA, nrow(pars))
  
  pars = pars[pars[,1] > tol_p, ]
  
  g = g +
    geom_function(fun = dist_mixture,
                  args = list(dist = "normal",
                              pars = pars),
                  alpha = transparency,
                  colour = colour_range[posterior::as_draws_df(fit)$.chain[i]])
}
g
```

